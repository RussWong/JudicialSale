## 特征选择

特征选择是指对数据的特征进行筛选,剔除一些重要性较低的特征,降低模型训练复杂度,同时提高模型性能的一种特征处理方法.

特征选择包括以下两部分:

+ 特征排序:选择sklearn工具包中的特征选择方式,对特征进行排序,每个模型返回一组特征重要性排序.常见的特征选择方式有以下几种:
  - f_regression:以方差分析为基础,对特征进行排序
  - mutual_info_regression:估计目标变量的互信息.两个随机变量之间的互信息是非负的,以度量变量之间的相关性.当且仅当两个随机变量相互独立时,其值为零.并且,其值越高,意味着相关性越高.
  - RFE+LR:递归特征消除法使用线性回归作为外部模型
  - RFE+LassoCV:递归特征消除法使用Lasso回归作为外部模型
  - RFE+RidgeCV:递归特征消除法使用岭回归作为外部模型
  - RFE+RFR:递归特征消除法使用随机森林作为外部模型
+ 特征删除:通过特征选择算法获得了相应的特征排序,采取投票法判断特征是否要保留.假设要保留的最终特征数为N,对每个特征给出赋值:0(表示特征排序不在前N位)或1(表示特征排序在前N位).最后根据每个特征所获赋值进行求和后排序,最终保留前N个特征.当出现赋值之和相同时,看分排名总和,值靠前的优先选择.

